{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough of model deployment as ML web service on Kubernetes\n",
    "\n",
    "This notebook outlines steps for deploying a machine learning model as a simple custom-built REST API prediction service to a Kubernetes instance.\n",
    "\n",
    "It is composed of the following sections:\n",
    " 1. Prepare environment:\n",
    " 2. Test the model\n",
    " 3. Run the service locally with Flask\n",
    " 4. Run the service using Docker\n",
    " 5. Run the service on a Kubernetes instance\n",
    "\n",
    "Note: this notebook assumes the user is running on a windows device and has the Docker, Kubectl and Helm CLIs installed. Alternative Curl command syntax would be needed for a linux-user.\n",
    "\n",
    "\n",
    "### Prepare environment\n",
    "\n",
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load, Loader\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import requests\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load config and chosen models**\n",
    "\n",
    "Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml','r') as config_file:\n",
    "    config = load(config_file, Loader=Loader)\n",
    "\n",
    "docker_registry = config['DOCKER_REGISTRY']\n",
    "service_name = config['SERVICE_NAME']\n",
    "api_version = config['API_VERSION']\n",
    "model_repo = '..\\experimentation\\models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy latest model to deployment directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 file(s) copied.\n"
     ]
    }
   ],
   "source": [
    "latest_model = sorted(os.listdir(model_repo))[-1]\n",
    "latest_model_path = os.path.join(model_repo,latest_model)\n",
    "\n",
    "!copy \"{latest_model_path}\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "Import data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../experimentation/datasets/test.csv\")\n",
    "test_entry = test_df[test_df.Fare.notna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the ML model and call the predict method on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in file with .pkl extension as the model\n",
    "ml_model = joblib.load(glob.glob('*.pkl')[0])\n",
    "predictions = ml_model.predict(test_entry)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the prediction service locally using Flask\n",
    "\n",
    "Run the flask app. The service will be served at http://127.0.0.1:5000/titanic/v0.0.1/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# run the service locally\n",
    "!python api.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the web service using Curl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will need to be run from a separate kernel / terminal \n",
    "!curl -X POST -H \"Content-Type:application/json\" --data \"{\\\"PassengerId\\\":[892],\\\"Pclass\\\":[3],\\\"Name\\\":[\\\"Kelly, Mr. James\\\"],\\\"Sex\\\":[\\\"male\\\"],\\\"Age\\\":[34.5],\\\"SibSp\\\":[0],\\\"Parch\\\":[0],\\\"Fare\\\":[7.8292],\\\"Embarked\\\":[\\\"S\\\"]}\" http://127.0.0.1:5000/titanic/v0.0.1/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containerise the prediction service using Docker\n",
    "\n",
    "**Build the docker image**\n",
    "\n",
    "Create a relevant tag that includes the image repository, a name for the service and its version. Build the image and tag it with the relevant tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 sha256:3653b7f4eb55c89c4ca666c0fefffb0333f8f8ac5ee2edfbcbb32b34f45053ee\n",
      "#1 transferring dockerfile: 32B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 sha256:d45d1a578aaa317b91817517a57aedad5a13ca5c8f968a3ebcf9160a01480f57\n",
      "#2 transferring context: 2B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/python:3.9-slim\n",
      "#3 sha256:3425157df499c84dd49181e5611a11caeed16adf15a5ddbcfa4c3002c56d3d27\n",
      "#3 DONE 1.5s\n",
      "\n",
      "#4 [1/5] FROM docker.io/library/python:3.9-slim@sha256:f4efbe5d1eb52c221fded79ddf18e4baa0606e7766afe2f07b0b330a9e79564a\n",
      "#4 sha256:9ce0d84a404c9ac604ef98baa1f1065d5a70e321684b314c01df3d72c5a89693\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#6 [internal] load build context\n",
      "#6 sha256:98489ef9bda79a123558bad715b9a37b5e733c2152a44f8c85f7d73a65b6d3a9\n",
      "#6 transferring context: 210B 0.0s done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "#5 [2/5] RUN mkdir /app\n",
      "#5 sha256:f8977e52fc2da4995e347b7fb878eedc812cb1c54dc0d42a662aa3db7b518aba\n",
      "#5 CACHED\n",
      "\n",
      "#7 [3/5] COPY config.yaml api.py *.pkl requirements.txt /app/\n",
      "#7 sha256:a3a1cdf1d4cdcac72ecb19030397b8d0fff595cbc35145d2f1e4972ab49d6049\n",
      "#7 DONE 0.1s\n",
      "\n",
      "#8 [4/5] WORKDIR /app\n",
      "#8 sha256:66c932f9b1161365c08dec3fa4d2399630b02a709158109b497715d96a4a540f\n",
      "#8 DONE 0.0s\n",
      "\n",
      "#9 [5/5] RUN pip install -r requirements.txt\n",
      "#9 sha256:c4f004bd1f12cd29964d05de2004354db3e01997e0396b2a04d201bf9e52c625\n",
      "#9 2.893 Collecting Flask==2.0.2\n",
      "#9 3.217   Downloading Flask-2.0.2-py3-none-any.whl (95 kB)\n",
      "#9 3.425 Collecting joblib==1.0.1\n",
      "#9 3.462   Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "#9 4.182 Collecting pandas==1.3.3\n",
      "#9 4.280   Downloading pandas-1.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "#9 6.691 Collecting PyYAML==6.0\n",
      "#9 6.750   Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "#9 6.960 Collecting sklearn==0.0\n",
      "#9 7.011   Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "#9 7.698 Collecting itsdangerous>=2.0\n",
      "#9 7.756   Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n",
      "#9 7.888 Collecting Jinja2>=3.0\n",
      "#9 7.955   Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
      "#9 8.115 Collecting Werkzeug>=2.0\n",
      "#9 8.177   Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "#9 8.388 Collecting click>=7.1.2\n",
      "#9 8.462   Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "#9 9.569 Collecting numpy>=1.17.3\n",
      "#9 9.652   Downloading numpy-1.21.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "#9 12.83 Collecting pytz>=2017.3\n",
      "#9 12.88   Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "#9 13.14 Collecting python-dateutil>=2.7.3\n",
      "#9 13.21   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "#9 13.90 Collecting scikit-learn\n",
      "#9 13.94   Downloading scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "#9 19.67 Collecting MarkupSafe>=2.0\n",
      "#9 19.74   Downloading MarkupSafe-2.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n",
      "#9 19.88 Collecting six>=1.5\n",
      "#9 20.08   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "#9 20.36 Collecting threadpoolctl>=2.0.0\n",
      "#9 20.41   Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "#9 20.87 Collecting scipy>=1.1.0\n",
      "#9 20.93   Downloading scipy-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "#9 29.27 Building wheels for collected packages: sklearn\n",
      "#9 29.28   Building wheel for sklearn (setup.py): started\n",
      "#9 29.91   Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "#9 29.91   Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=9fe21e7403f0f739046cc94009c4004edea5255b34bc1631668cea7dc2ddaa2a\n",
      "#9 29.91   Stored in directory: /root/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "#9 29.91 Successfully built sklearn\n",
      "#9 30.07 Installing collected packages: numpy, threadpoolctl, six, scipy, MarkupSafe, joblib, Werkzeug, scikit-learn, pytz, python-dateutil, Jinja2, itsdangerous, click, sklearn, PyYAML, pandas, Flask\n",
      "#9 43.19 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "#9 43.19 Successfully installed Flask-2.0.2 Jinja2-3.0.3 MarkupSafe-2.0.1 PyYAML-6.0 Werkzeug-2.0.2 click-8.0.3 itsdangerous-2.0.1 joblib-1.0.1 numpy-1.21.5 pandas-1.3.3 python-dateutil-2.8.2 pytz-2021.3 scikit-learn-1.0.2 scipy-1.7.3 six-1.16.0 sklearn-0.0 threadpoolctl-3.0.0\n",
      "#9 43.56 WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "#9 43.56 You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "#9 DONE 43.9s\n",
      "\n",
      "#10 exporting to image\n",
      "#10 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n",
      "#10 exporting layers\n",
      "#10 exporting layers 2.5s done\n",
      "#10 writing image sha256:8c858388035c7d613239701544098c42b25f4ec1c25787f131887698de1ea358 done\n",
      "#10 naming to docker.io/edlongbottom/mlwebservice/titanic:0.0.1 done\n",
      "#10 DONE 2.5s\n",
      "\n",
      "Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n"
     ]
    }
   ],
   "source": [
    "tag = f'{docker_registry}/{service_name}:{api_version}'\n",
    "!docker build -t {tag} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the service on Docker**\n",
    "\n",
    "Run the image as a container locally and map container port 5000 to localhost port 5000 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm -p 5000:5000 --name test-ml-model edlongbottom/mlwebservice/titanic:0.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the service**\n",
    "\n",
    "Use Curl or the python requests module to test the prediction web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   167  100    20  100   147     20    147  0:00:01 --:--:--  0:00:01  163k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\":[0]}\n"
     ]
    }
   ],
   "source": [
    "# again, this must be executed from a separate kernel/terminal as the kernel is occupied running the previous cell\n",
    "!curl -X POST -H \"Content-Type:application/json\" --data \"{\\\"PassengerId\\\":[892],\\\"Pclass\\\":[3],\\\"Name\\\":[\\\"Kelly, Mr. James\\\"],\\\"Sex\\\":[\\\"male\\\"],\\\"Age\\\":[34.5],\\\"SibSp\\\":[0],\\\"Parch\\\":[0],\\\"Fare\\\":[7.8292],\\\"Embarked\\\":[\\\"S\\\"]}\" http://127.0.0.1:5000/titanic/v0.0.1/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tear down**\n",
    "\n",
    "Once testing is complete, stop and remove the docker container. This step isn't required if the '--rm' flag was included when performing the docker run step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop test-ml-model\n",
    "!docker rm test-ml-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the prediction service to Kubernetes\n",
    "\n",
    "Push the built image to Docker hub so it available remotely (you may need to log in to Docker first and create the repository if you haven't already)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push {tag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure a kubernetes cluster** \n",
    "\n",
    "At this point, a kubernetes cluster is required and your kubectl CLI must be configured to set the chosen cluster as its current context. Docker desktop or Minikube can be used to spin up a cluster locally, or alternatively you could look to provision a cluster through a cloud provide (for example, AKS from Azure).\n",
    "\n",
    "**Deploy the prediction service using Helm**\n",
    "\n",
    "Once you have a cluster setup and you are connected to it, deploy the docker image to Kubernetes using Helm. The helm chart is included under the deployment folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"mlwebservice-titanic\" does not exist. Installing it now.\n",
      "NAME: mlwebservice-titanic\n",
      "LAST DEPLOYED: Thu Dec 30 15:54:52 2021\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install mlwebservice-titanic helm-ml-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the deployment was successful by checking the pods in the model-serving namespace (you may need to wait a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                    READY   STATUS    RESTARTS   AGE\n",
      "mlwebservice-titanic-6b8c7c5dcc-z4cmc   1/1     Running   0          58s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -n model-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the web service using Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\":[0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   167  100    20  100   147     20    147  0:00:01 --:--:--  0:00:01  1532\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST -H \"Content-Type:application/json\" --data \"{\\\"PassengerId\\\":[892],\\\"Pclass\\\":[3],\\\"Name\\\":[\\\"Kelly, Mr. James\\\"],\\\"Sex\\\":[\\\"male\\\"],\\\"Age\\\":[34.5],\\\"SibSp\\\":[0],\\\"Parch\\\":[0],\\\"Fare\\\":[7.8292],\\\"Embarked\\\":[\\\"S\\\"]}\" http://127.0.0.1:5000/titanic/v0.0.1/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tear down**\n",
    "\n",
    "Remove the service when not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"mlwebservice-titanic\" uninstalled\n"
     ]
    }
   ],
   "source": [
    "!helm uninstall mlwebservice-titanic"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c54b4acbf7bec5da1dd6ed3afae36d1425463432e804fe22fdd7ec77f7461ae2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
