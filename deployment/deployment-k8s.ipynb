{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough of model deployment as ML web service on Kubernetes\n",
    "\n",
    "This notebook outlines steps for deploying a machine learning model as a simple custom-built REST API prediction service to a Kubernetes instance.\n",
    "\n",
    "It is composed of the following sections:\n",
    " 1. Prepare environment\n",
    " 2. Test the model\n",
    " 3. Run the service locally with Flask\n",
    " 4. Run the service using Docker\n",
    " 5. Run the service on a Kubernetes instance\n",
    " 6. Autoscaling and load-testing the service on Kubernetes\n",
    "\n",
    "Note: this notebook assumes the user is running on a windows device and has the Docker, Kubectl and Helm CLIs installed. Alternative Curl command syntax would be needed for a linux-user.\n",
    "\n",
    "\n",
    "### Prepare environment\n",
    "\n",
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load, Loader\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "import requests\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load config and chosen models**\n",
    "\n",
    "Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml','r') as config_file:\n",
    "    config = load(config_file, Loader=Loader)\n",
    "\n",
    "docker_registry = config['DOCKER_REGISTRY']\n",
    "service_name = config['SERVICE_NAME']\n",
    "api_version = config['API_VERSION']\n",
    "model_repo = '..\\experimentation\\models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy latest model to deployment directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 file(s) copied.\n"
     ]
    }
   ],
   "source": [
    "latest_model = sorted(os.listdir(model_repo))[-1]\n",
    "latest_model_path = os.path.join(model_repo,latest_model)\n",
    "\n",
    "!copy \"{latest_model_path}\" ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "Import data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../experimentation/datasets/test.csv\")\n",
    "test_entry = test_df[test_df.Fare.notna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the ML model and call the predict method on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in file with .pkl extension as the model\n",
    "ml_model = joblib.load(glob.glob('*.pkl')[0])\n",
    "predictions = ml_model.predict(test_entry)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the prediction service locally using Flask\n",
    "\n",
    "Run the flask app. The service will be served at http://127.0.0.1:5000/titanic/v0.0.1/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# run the service locally\n",
    "!python api.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, run the service from a different command prompt / shell, and test the web service using Curl here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\":[0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   167  100    20  100   147   1435  10551 --:--:-- --:--:-- --:--:-- 12846\n"
     ]
    }
   ],
   "source": [
    "# this will need to be run from a separate kernel / terminal to that running the web service\n",
    "!curl -X POST -H \"Content-Type:application/json\" --data \"{\\\"PassengerId\\\":[892],\\\"Pclass\\\":[3],\\\"Name\\\":[\\\"Kelly, Mr. James\\\"],\\\"Sex\\\":[\\\"male\\\"],\\\"Age\\\":[34.5],\\\"SibSp\\\":[0],\\\"Parch\\\":[0],\\\"Fare\\\":[7.8292],\\\"Embarked\\\":[\\\"S\\\"]}\" http://127.0.0.1:5000/titanic/v0.0.1/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containerise the prediction service using Docker\n",
    "\n",
    "**Build the docker image**\n",
    "\n",
    "Create a relevant tag that includes the image repository, a name for the service and its version. Build the image and tag it with the relevant tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 sha256:3653b7f4eb55c89c4ca666c0fefffb0333f8f8ac5ee2edfbcbb32b34f45053ee\n",
      "#1 transferring dockerfile: 32B done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 sha256:d45d1a578aaa317b91817517a57aedad5a13ca5c8f968a3ebcf9160a01480f57\n",
      "#2 transferring context: 2B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/python:3.9-slim\n",
      "#3 sha256:3425157df499c84dd49181e5611a11caeed16adf15a5ddbcfa4c3002c56d3d27\n",
      "#3 DONE 1.5s\n",
      "\n",
      "#4 [1/5] FROM docker.io/library/python:3.9-slim@sha256:f4efbe5d1eb52c221fded79ddf18e4baa0606e7766afe2f07b0b330a9e79564a\n",
      "#4 sha256:9ce0d84a404c9ac604ef98baa1f1065d5a70e321684b314c01df3d72c5a89693\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#6 [internal] load build context\n",
      "#6 sha256:98489ef9bda79a123558bad715b9a37b5e733c2152a44f8c85f7d73a65b6d3a9\n",
      "#6 transferring context: 210B 0.0s done\n",
      "#6 DONE 0.0s\n",
      "\n",
      "#5 [2/5] RUN mkdir /app\n",
      "#5 sha256:f8977e52fc2da4995e347b7fb878eedc812cb1c54dc0d42a662aa3db7b518aba\n",
      "#5 CACHED\n",
      "\n",
      "#7 [3/5] COPY config.yaml api.py *.pkl requirements.txt /app/\n",
      "#7 sha256:a3a1cdf1d4cdcac72ecb19030397b8d0fff595cbc35145d2f1e4972ab49d6049\n",
      "#7 DONE 0.1s\n",
      "\n",
      "#8 [4/5] WORKDIR /app\n",
      "#8 sha256:66c932f9b1161365c08dec3fa4d2399630b02a709158109b497715d96a4a540f\n",
      "#8 DONE 0.0s\n",
      "\n",
      "#9 [5/5] RUN pip install -r requirements.txt\n",
      "#9 sha256:c4f004bd1f12cd29964d05de2004354db3e01997e0396b2a04d201bf9e52c625\n",
      "#9 2.893 Collecting Flask==2.0.2\n",
      "#9 3.217   Downloading Flask-2.0.2-py3-none-any.whl (95 kB)\n",
      "#9 3.425 Collecting joblib==1.0.1\n",
      "#9 3.462   Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "#9 4.182 Collecting pandas==1.3.3\n",
      "#9 4.280   Downloading pandas-1.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "#9 6.691 Collecting PyYAML==6.0\n",
      "#9 6.750   Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "#9 6.960 Collecting sklearn==0.0\n",
      "#9 7.011   Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "#9 7.698 Collecting itsdangerous>=2.0\n",
      "#9 7.756   Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n",
      "#9 7.888 Collecting Jinja2>=3.0\n",
      "#9 7.955   Downloading Jinja2-3.0.3-py3-none-any.whl (133 kB)\n",
      "#9 8.115 Collecting Werkzeug>=2.0\n",
      "#9 8.177   Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "#9 8.388 Collecting click>=7.1.2\n",
      "#9 8.462   Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "#9 9.569 Collecting numpy>=1.17.3\n",
      "#9 9.652   Downloading numpy-1.21.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "#9 12.83 Collecting pytz>=2017.3\n",
      "#9 12.88   Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "#9 13.14 Collecting python-dateutil>=2.7.3\n",
      "#9 13.21   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "#9 13.90 Collecting scikit-learn\n",
      "#9 13.94   Downloading scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "#9 19.67 Collecting MarkupSafe>=2.0\n",
      "#9 19.74   Downloading MarkupSafe-2.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (30 kB)\n",
      "#9 19.88 Collecting six>=1.5\n",
      "#9 20.08   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "#9 20.36 Collecting threadpoolctl>=2.0.0\n",
      "#9 20.41   Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "#9 20.87 Collecting scipy>=1.1.0\n",
      "#9 20.93   Downloading scipy-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "#9 29.27 Building wheels for collected packages: sklearn\n",
      "#9 29.28   Building wheel for sklearn (setup.py): started\n",
      "#9 29.91   Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "#9 29.91   Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=9fe21e7403f0f739046cc94009c4004edea5255b34bc1631668cea7dc2ddaa2a\n",
      "#9 29.91   Stored in directory: /root/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "#9 29.91 Successfully built sklearn\n",
      "#9 30.07 Installing collected packages: numpy, threadpoolctl, six, scipy, MarkupSafe, joblib, Werkzeug, scikit-learn, pytz, python-dateutil, Jinja2, itsdangerous, click, sklearn, PyYAML, pandas, Flask\n",
      "#9 43.19 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "#9 43.19 Successfully installed Flask-2.0.2 Jinja2-3.0.3 MarkupSafe-2.0.1 PyYAML-6.0 Werkzeug-2.0.2 click-8.0.3 itsdangerous-2.0.1 joblib-1.0.1 numpy-1.21.5 pandas-1.3.3 python-dateutil-2.8.2 pytz-2021.3 scikit-learn-1.0.2 scipy-1.7.3 six-1.16.0 sklearn-0.0 threadpoolctl-3.0.0\n",
      "#9 43.56 WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "#9 43.56 You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
      "#9 DONE 43.9s\n",
      "\n",
      "#10 exporting to image\n",
      "#10 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n",
      "#10 exporting layers\n",
      "#10 exporting layers 2.5s done\n",
      "#10 writing image sha256:8c858388035c7d613239701544098c42b25f4ec1c25787f131887698de1ea358 done\n",
      "#10 naming to docker.io/edlongbottom/mlwebservice/titanic:0.0.1 done\n",
      "#10 DONE 2.5s\n",
      "\n",
      "Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n"
     ]
    }
   ],
   "source": [
    "tag = f'{docker_registry}/{service_name}:{api_version}'\n",
    "!docker build -t {tag} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the service on Docker**\n",
    "\n",
    "Run the image as a container locally and map container port 5000 to localhost port 5000 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!docker run --rm -p 5000:5000 --name test-ml-model edlongbottom/mlwebservice/titanic:0.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the service**\n",
    "\n",
    "Use Curl or the python requests module to test the prediction web service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\":[0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   167  100    20  100   147   1279   9403 --:--:-- --:--:-- --:--:-- 11928\n"
     ]
    }
   ],
   "source": [
    "# again, this must be executed from a separate kernel/terminal as the kernel is occupied running the previous cell\n",
    "!curl -X POST -H \"Content-Type:application/json\" --data \"{\\\"PassengerId\\\":[892],\\\"Pclass\\\":[3],\\\"Name\\\":[\\\"Kelly, Mr. James\\\"],\\\"Sex\\\":[\\\"male\\\"],\\\"Age\\\":[34.5],\\\"SibSp\\\":[0],\\\"Parch\\\":[0],\\\"Fare\\\":[7.8292],\\\"Embarked\\\":[\\\"S\\\"]}\" http://127.0.0.1:5000/titanic/v0.0.1/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tear down**\n",
    "\n",
    "Once testing is complete, stop and remove the docker container. This step isn't required if the '--rm' flag was included when performing the docker run step.\n",
    "\n",
    "It will throw an error if you used the '--rm' flag as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-ml-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: No such container: test-ml-model\n"
     ]
    }
   ],
   "source": [
    "!docker stop test-ml-model\n",
    "!docker rm test-ml-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the prediction service to Kubernetes\n",
    "\n",
    "Push the built image to Docker hub so it available remotely (you may need to log in to Docker first and create the repository if you haven't already)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push {tag}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure a kubernetes cluster** \n",
    "\n",
    "At this point, a kubernetes cluster is required and your kubectl CLI must be configured to set the chosen cluster as its current context. Docker desktop or Minikube can be used to spin up a cluster locally, or alternatively you could look to provision a cluster through a cloud provide (for example, AKS from Azure).\n",
    "\n",
    "I am using Minikube here, which needs to be started to spin up a kubernetes cluster:\n",
    "\n",
    "`minikube start`\n",
    "\n",
    "I can then use the kubectl CLI to run some basic commands to check the cluster is running ok:\n",
    " - the first command provides information on the node\n",
    " - the second command provides info on all kubernetes objects in all namespaces on this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME       STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION                      CONTAINER-RUNTIME\n",
      "minikube   Ready    control-plane,master   22h   v1.22.3   192.168.49.2   <none>        Ubuntu 20.04.2 LTS   5.10.16.3-microsoft-standard-WSL2   docker://20.10.8\n",
      "\n",
      "\n",
      "NAMESPACE              NAME                                             READY   STATUS    RESTARTS        AGE\n",
      "kube-system            pod/coredns-78fcd69978-fph8x                     1/1     Running   2 (4m55s ago)   22h\n",
      "kube-system            pod/etcd-minikube                                1/1     Running   2 (4m55s ago)   22h\n",
      "kube-system            pod/kube-apiserver-minikube                      1/1     Running   2 (4m55s ago)   22h\n",
      "kube-system            pod/kube-controller-manager-minikube             1/1     Running   2 (4m55s ago)   22h\n",
      "kube-system            pod/kube-proxy-n9dvx                             1/1     Running   2 (4m55s ago)   22h\n",
      "kube-system            pod/kube-scheduler-minikube                      1/1     Running   2 (4m55s ago)   22h\n",
      "kube-system            pod/metrics-server-77c99ccb96-4zmff              1/1     Running   4 (4m55s ago)   22h\n",
      "kube-system            pod/storage-provisioner                          1/1     Running   8 (3m53s ago)   22h\n",
      "kubernetes-dashboard   pod/dashboard-metrics-scraper-5594458c94-vmk4f   1/1     Running   1 (4m55s ago)   21h\n",
      "kubernetes-dashboard   pod/kubernetes-dashboard-654cf69797-khjtn        1/1     Running   2 (3m49s ago)   21h\n",
      "\n",
      "NAMESPACE              NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE\n",
      "default                service/kubernetes                  ClusterIP   10.96.0.1        <none>        443/TCP                  22h\n",
      "kube-system            service/kube-dns                    ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP   22h\n",
      "kube-system            service/metrics-server              ClusterIP   10.110.57.33     <none>        443/TCP                  22h\n",
      "kubernetes-dashboard   service/dashboard-metrics-scraper   ClusterIP   10.105.137.155   <none>        8000/TCP                 21h\n",
      "kubernetes-dashboard   service/kubernetes-dashboard        ClusterIP   10.107.77.133    <none>        80/TCP                   21h\n",
      "\n",
      "NAMESPACE     NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n",
      "kube-system   daemonset.apps/kube-proxy   1         1         1       1            1           kubernetes.io/os=linux   22h\n",
      "\n",
      "NAMESPACE              NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "kube-system            deployment.apps/coredns                     1/1     1            1           22h\n",
      "kube-system            deployment.apps/metrics-server              1/1     1            1           22h\n",
      "kubernetes-dashboard   deployment.apps/dashboard-metrics-scraper   1/1     1            1           21h\n",
      "kubernetes-dashboard   deployment.apps/kubernetes-dashboard        1/1     1            1           21h\n",
      "\n",
      "NAMESPACE              NAME                                                   DESIRED   CURRENT   READY   AGE\n",
      "kube-system            replicaset.apps/coredns-78fcd69978                     1         1         1       22h\n",
      "kube-system            replicaset.apps/metrics-server-77c99ccb96              1         1         1       22h\n",
      "kubernetes-dashboard   replicaset.apps/dashboard-metrics-scraper-5594458c94   1         1         1       21h\n",
      "kubernetes-dashboard   replicaset.apps/kubernetes-dashboard-654cf69797        1         1         1       21h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes -o wide\n",
    "print(\"\\n\")\n",
    "!kubectl get all --all-namespaces     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deploy the prediction service using Helm**\n",
    "\n",
    "Once you have a cluster setup and you are connected to it, deploy the docker image to Kubernetes using Helm. The helm chart is included under the deployment folder. Let's spin up a single instance of the web service first (using the helm-ml-serving-single chart).\n",
    "\n",
    "This chart includes the following files, which collectively create Namespace, LoadBalancer and Deployment objects on Kubernetes:\n",
    "\n",
    " - `templates/deployment.yaml`\n",
    " - `templates/service.yaml`\n",
    " - `templates/namespace.yaml`\n",
    " - `Chart.yaml`\n",
    " - `values.yaml`\n",
    " \n",
    "These objects could be deployed one by one declaratively as YAML files. However, things can get complicted when you have complex applications with many objects and services that all reference each other. Helm is useful as a package manager, keeping all variables in one file (values.yaml) and version information in another (Chart.yaml). The Helm CLI is used to deploy, upgrade and rollback charts.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"mlwebservice-titanic\" does not exist. Installing it now.\n",
      "NAME: mlwebservice-titanic\n",
      "LAST DEPLOYED: Thu Jan 20 15:14:28 2022\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 1\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install mlwebservice-titanic helm-ml-serving-single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the deployment was successful by checking the pods in the model-serving namespace (you may need to wait a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                               READY   STATUS    RESTARTS   AGE\n",
      "mlwebservice-titanic-deployment-6b8c7c5dcc-hmghx   1/1     Running   0          2m48s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -n model-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the web service using Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\":[0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   167  100    20  100   147   1223   8995 --:--:-- --:--:-- --:--:-- 11133\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST -H \"Content-Type:application/json\" --data \"{\\\"PassengerId\\\":[892],\\\"Pclass\\\":[3],\\\"Name\\\":[\\\"Kelly, Mr. James\\\"],\\\"Sex\\\":[\\\"male\\\"],\\\"Age\\\":[34.5],\\\"SibSp\\\":[0],\\\"Parch\\\":[0],\\\"Fare\\\":[7.8292],\\\"Embarked\\\":[\\\"S\\\"]}\" http://127.0.0.1:5000/titanic/v0.0.1/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tear down**\n",
    "\n",
    "Remove the service when not in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"mlwebservice-titanic\" uninstalled\n"
     ]
    }
   ],
   "source": [
    "!helm uninstall mlwebservice-titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce autoscaling and stress-test the service\n",
    "\n",
    "Kubernetes has a Horizontal Pod Autoscaler (HPA) to allow you to scale up the number of pods to meet demands on the application. Depending on the resource requirements of your pods and the spec of the node, you may also want to introduce more nodes to cope with demand. Here, we will look at scaling the number of pods only.\n",
    "\n",
    "**Enable Metrics Server**\n",
    "\n",
    "HPA queries a Metrics Server to measure resource utilisation such as CPU / RAM. Minikube has a metrics server that must be launched using the following command. It will launch a deployment object which you can view using kubectl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Using image k8s.gcr.io/metrics-server/metrics-server:v0.4.2\n",
      "* The 'metrics-server' addon is enabled\n"
     ]
    }
   ],
   "source": [
    "!minikube addons enable metrics-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "metrics-server   1/1     1            1           22h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployment metrics-server -n kube-system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deploy the prediction service**\n",
    "\n",
    "We can now deploy the second version of our helm chart which includes `autoscale.yaml` which will create a HPA object. This will allow the deployment to scale between 1 and 3 replicas of the model based on CPU utilization. The `deployment.yaml` has also been amended to include resource requests and limits for the pod. Keeping this low will force Kubernetes to scale up the number of pods as the limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release \"mlwebservice-titanic\" has been upgraded. Happy Helming!\n",
      "NAME: mlwebservice-titanic\n",
      "LAST DEPLOYED: Thu Jan 20 16:51:31 2022\n",
      "NAMESPACE: default\n",
      "STATUS: deployed\n",
      "REVISION: 4\n",
      "TEST SUITE: None\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install mlwebservice-titanic helm-ml-serving-multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use kubectl to get information on the auto-scaler and the load-balancer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       REFERENCE                                    TARGETS    MINPODS   MAXPODS   REPLICAS   AGE\n",
      "mlwebservice-titanic-hpa   Deployment/mlwebservice-titanic-deployment   200%/20%   1         3         1          19s\n"
     ]
    }
   ],
   "source": [
    "!kubectl get hpa -n model-serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n",
      "service-mlwebservice-titanic   LoadBalancer   10.109.149.55   <pending>     5000:32276/TCP   82m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc -n model-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Minikube locally, run the following command to set up a tunnel from a random port on localhost to the loadbalancer: \n",
    "\n",
    "`minikube service service-mlwebservice-titanic -n model-serving`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, send a request using the port which was assigned (mine was `58100`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [0]}\n"
     ]
    }
   ],
   "source": [
    "# define base URL (localhost +  port)\n",
    "url = \"http://127.0.0.1:5000/titanic/v0.0.1/predict\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "body = {'PassengerId':[892],'Pclass':[3],'Name':['Kelly, Mr. James'],'Sex':['male'],\n",
    "        'Age':[34.5],'SibSp':[0],'Fare':[7.8292],'Embarked':['S']}\n",
    "\n",
    "# send a get request to flask api\n",
    "response = requests.post(url=url, data=json.dumps(body), headers=headers)\n",
    "print(response.json()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stress test the service**\n",
    "\n",
    "The service can be tested by repeatedly sending requests at it to simulate traffic. See `stress-test.py` for a script that uses a while loop and the requests module to fire requests at the prediction service.\n",
    "\n",
    "First, run the dashboard so you can monitor the deployment and how it scales as it is subject to traffic:\n",
    "\n",
    "`minikube dashboard`\n",
    "\n",
    "Then, run the python script to test the service:\n",
    "\n",
    "`python stress-test.py`\n",
    "\n",
    "By viewing the dashboard, you should be able to see the number of pods increase to meet the demand from the stress test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tear down resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"mlwebservice-titanic\" uninstalled\n"
     ]
    }
   ],
   "source": [
    "!helm uninstall mlwebservice-titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Deleting \"minikube\" in docker ...\n",
      "* Deleting container \"minikube\" ...\n",
      "* Removing C:\\Users\\eddlo\\.minikube\\machines\\minikube ...\n",
      "* Removed all traces of the \"minikube\" cluster.\n"
     ]
    }
   ],
   "source": [
    "!minikube delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load testing using Locus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c54b4acbf7bec5da1dd6ed3afae36d1425463432e804fe22fdd7ec77f7461ae2"
  },
  "kernelspec": {
   "display_name": "venv-titanic",
   "language": "python",
   "name": "venv-titanic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
