{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1759aa3",
   "metadata": {},
   "source": [
    "# Titanic dataset: classification\n",
    "\n",
    "### Description\n",
    "\n",
    "This notebook sets out an approach for building a survival-classifier predictive model for the Titanic disaster. Various models will be trained and evaluated using data from Kaggle. The best-performer will then be deployed for use. This follows on from the *Titanic_EDA.ipynb* notebook where exploratory data analysis of the titantic dataset is conducted. \n",
    "\n",
    "Goal: create a machine learning model to generate predictions for whether an individual will survive the titanic disaster.\n",
    "\n",
    "\n",
    "\n",
    "**Feature descriptors:**\n",
    " - Pclass: ticket class\n",
    " - Name: full name of passenger\n",
    " - Sex: sex (m/f)\n",
    " - Age: age in years\n",
    " - SibSp: # of siblings/spouses aboard\n",
    " - Parch: # of parents / children aboard the Titanic\n",
    " - Ticket: ticket number\n",
    " - Fare: passenger fare\n",
    " - Cabin: cabin number\n",
    " - Embarked: port of embarkation\n",
    " \n",
    " \n",
    "**Target:** Survival - whether the individual survived (0 - No, 1 - Yes)\n",
    "\n",
    "### Prepare environment\n",
    "\n",
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa2a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, minmax_scale, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a72b6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c010e",
   "metadata": {},
   "source": [
    "**Load datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bdbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../datasets/train.csv\")\n",
    "test_df = pd.read_csv(\"../datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19278e6a",
   "metadata": {},
   "source": [
    "**Explore training data**\n",
    "\n",
    "See Titanic_EDA.ipynb for a more in-depth EDA exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f26e1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first 10 rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33533008",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "Refer to EDA notebook for more in-depth analysis of the data and reasoning for the steps applied made below in feature selection and transformation. \n",
    "\n",
    "**Feature selection**\n",
    "\n",
    "The name and ticket columns should be dropped as they contain all unique values and appear unlikely to be useful to the model. The cabin column has a large number of nan values and so this should also be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7cea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['PassengerId','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "target_col = 'Survived'\n",
    "\n",
    "X = train_df.copy()\n",
    "y = train_df[[target_col]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8519346",
   "metadata": {},
   "source": [
    "Firstly, train-test split the data with stratification to ensure there is enough target examples in both training and test sets. We want to split the data ahead of performing any data pre-processing so that we can check any pipeline we construct from the training data against the test part of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b63f4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=324, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0efc02",
   "metadata": {},
   "source": [
    "**Data cleaning and transformations**\n",
    "\n",
    "There are 177 missing values in the Age column. Due to the small dataset size, we shouldn't remove all these rows. Instead use an imputer to replace missing values with a mean for the column. Impute missing values in Embarked with a new category 'Unknown'.\n",
    "\n",
    "Any fields containing continuous numeric data should be scaled or normalized. Use One Hot Encoding (OHE) to convert the categoric variables (sex, embarked, Pclass, SibSp) so each category is a feature with values (1/0). Parch has not been included as categories in training and testing datasets are not consistent which will cause issues for OHE.\n",
    "\n",
    "Combine all these transformations into an sklearn pipeline for repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c04406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer()),\n",
       "                                                 ('standardscaler',\n",
       "                                                  StandardScaler())]),\n",
       "                                 ['Age']),\n",
       "                                ('pipeline-2',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(fill_value='UNKNOWN',\n",
       "                                                                strategy='constant')),\n",
       "                                                 ('onehotencoder',\n",
       "                                                  OneHotEncoder())]),\n",
       "                                 ['Embarked']),\n",
       "                                ('standardscaler', StandardScaler(), ['Fare']),\n",
       "                                ('onehotencoder', OneHotEncoder(),\n",
       "                                 ['Sex', 'Pclass', 'SibSp'])])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct pipeline of transformtions\n",
    "feature_pipe = make_column_transformer(\n",
    "    (make_pipeline(SimpleImputer(strategy='mean'), StandardScaler()), ['Age']),\n",
    "    (make_pipeline(SimpleImputer(strategy='constant', fill_value='UNKNOWN'),OneHotEncoder()), ['Embarked']),\n",
    "    (StandardScaler(), ['Fare']),\n",
    "    (OneHotEncoder(),['Sex','Pclass','SibSp']))\n",
    "\n",
    "# remove the rows with missing values in the Embarked column then fit the transformation pipeline with the training data\n",
    "feature_pipe.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9faa5b",
   "metadata": {},
   "source": [
    "Apply the pipeline to both train and test parts of the supervised training data split. Also apply it to the testing dataset from Kaggle to confirm that the pipeline is robust.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3213536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply feature pipeline tranformations to train and test sets to check no issues\n",
    "X_train_processed = feature_pipe.transform(X_train)\n",
    "X_test_processed = feature_pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af57f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pipeline to the test dataframe to check no issues\n",
    "test_df_processed = feature_pipe.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92682ffd",
   "metadata": {},
   "source": [
    "### Model training, validation and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0056ea4",
   "metadata": {},
   "source": [
    "Follow the training, validation and testing framework\n",
    " - Use 60% of the training data for model training\n",
    " - Use 20% for assessing model performance while fine-tuning parameters\n",
    " - Hold-out 20% for final evaluation\n",
    " \n",
    "Cross-validation should be applied to the 80% for training and fine-tuning using sklearn's GridSearchCV. This will also allow hyperparameter tuning. Train and validate each classifer on the training dataset using cross-validation.\n",
    "Accuracy is being used as the evaluation metric, with a dummy classifier to benchmark the evaluation.\n",
    "\n",
    "Create a function for consistent training, validation and testing throughout experimentation. This should also provide training and test set scores (giving useful info on bias/variance), and save the pipeline to the model respository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccbe7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = '../models'\n",
    "def model_train_val(model, feature_pipe, params, X_train, y_train, X_test, y_test, cv=4, scoring='accuracy'):\n",
    "    \n",
    "    # apply feature transformations\n",
    "    X_train_processed = feature_pipe.transform(X_train)\n",
    "    X_test_processed = feature_pipe.transform(X_test)\n",
    "    \n",
    "    # run gridsearch with cross-validation to find best estimator\n",
    "    clf = GridSearchCV(model, param_grid=params, cv=cv, scoring=scoring, return_train_score=True)\n",
    "    clf.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # print the results to screen\n",
    "    print(f'Best params: {clf.best_params_}')\n",
    "    print(f'Best CV score: {clf.best_score_}')\n",
    "    print(f\"Training set score: {clf.cv_results_['mean_train_score'][clf.best_index_]:2.3}\")\n",
    "    print(f'Test set score: {clf.best_estimator_.score(X_test_processed,y_test):2.3}')\n",
    "    \n",
    "    # construct final pipeline\n",
    "    predict_pipe = make_pipeline(feature_pipe, clf.best_estimator_)\n",
    "    \n",
    "    # save the model in the repository\n",
    "    joblib.dump(predict_pipe, os.path.join(model_repo, f'pipe_{str(clf.best_estimator_)}.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb639be",
   "metadata": {},
   "source": [
    "**Dummy classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d5a1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {}\n",
      "Best CV score: 0.6165730337078652\n",
      "Training set score: 0.617\n",
      "Test set score: 0.615\n"
     ]
    }
   ],
   "source": [
    "# use a dummy classifier to benchmark the evaluation\n",
    "params = {}\n",
    "model_train_val(DummyClassifier(strategy='most_frequent'), feature_pipe, params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9c613",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "262de373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_neighbors': 5}\n",
      "Best CV score: 0.803370786516854\n",
      "Training set score: 0.86\n",
      "Test set score: 0.827\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbors\n",
    "params = {'n_neighbors':range(2,11)}\n",
    "model_train_val(KNeighborsClassifier(), feature_pipe, params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330d75a",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a4b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 0.1, 'penalty': 'l2'}\n",
      "Best CV score: 0.7907303370786517\n",
      "Training set score: 0.8\n",
      "Test set score: 0.832\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "params = {'penalty':['none','l1','l2'],'C':[0.01,0.1,1,10]}\n",
    "model_train_val(LogisticRegression(), feature_pipe, params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621b968",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76a4069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 1, 'kernel': 'rbf'}\n",
      "Best CV score: 0.8174157303370787\n",
      "Training set score: 0.835\n",
      "Test set score: 0.849\n"
     ]
    }
   ],
   "source": [
    "# support vector machines\n",
    "params = {'kernel':('linear','rbf'),'C':(0.01,0.1,1)}\n",
    "model_train_val(SVC(), feature_pipe, params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db76d0",
   "metadata": {},
   "source": [
    "#### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b9ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 5, 'max_features': 6}\n",
      "Best CV score: 0.806179775280899\n",
      "Training set score: 0.818\n",
      "Test set score: 0.816\n"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "params = {'max_depth':range(1,11),'max_features':range(1,10)}\n",
    "model_train_val(DecisionTreeClassifier(random_state=0),feature_pipe, params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8f76f",
   "metadata": {},
   "source": [
    "#### Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce657028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 8, 'n_estimators': 41}\n",
      "Best CV score: 0.8160112359550562\n",
      "Training set score: 0.909\n",
      "Test set score: 0.832\n"
     ]
    }
   ],
   "source": [
    "# random forests\n",
    "params = {'n_estimators':range(1,101,10),'max_depth':range(1,11),}\n",
    "model_train_val(RandomForestClassifier(random_state=0),feature_pipe, params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d75e0",
   "metadata": {},
   "source": [
    "#### Gradient boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosted trees\n",
    "params= {'n_estimators':range(1,101,10),'max_depth':range(1,11)}\n",
    "model_train_val(GradientBoostingClassifier(random_state=0), feature_pipe, params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using XGBoost\n",
    "params= {'n_estimators':range(1,101,10),'max_depth':range(1,11),'max_features':range(1,10)}\n",
    "#model_train_val(XGBClassifier(random_state=0,use_label_encoder=False), params, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-titanic",
   "language": "python",
   "name": "venv-titanic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
