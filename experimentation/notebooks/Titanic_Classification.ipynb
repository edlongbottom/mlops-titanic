{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f7dc46",
   "metadata": {},
   "source": [
    "# Titanic dataset: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1759aa3",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This notebook sets out an approach for building a survival-classifier predictive model for the Titanic disaster. Various models will be trained and evaluated using data from Kaggle. The best-performer will then be deployed for use. \n",
    "\n",
    "Goal: create a machine learning model to generate predictions for whether an individual will survive the titanic disaster.\n",
    "\n",
    "\n",
    "\n",
    "**Feature descriptors:**\n",
    " - Pclass: ticket class\n",
    " - Name: full name of passenger\n",
    " - Sex: sex (m/f)\n",
    " - Age: age in years\n",
    " - SibSp: # of siblings/spouses aboard\n",
    " - Parch: # of parents / children aboard the Titanic\n",
    " - Ticket: ticket number\n",
    " - Fare: passenger fare\n",
    " - Cabin: cabin number\n",
    " - Embarked: port of embarkation\n",
    " \n",
    " \n",
    "Target: Survival - whether the individual survived (0 - No, 1 - Yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb76f05",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa2a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, minmax_scale\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a72b6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c010e",
   "metadata": {},
   "source": [
    "**Load datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58bdbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../datasets/train.csv\")\n",
    "test_df = pd.read_csv(\"../datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19278e6a",
   "metadata": {},
   "source": [
    "**Explore training data**\n",
    "\n",
    "see Titanic_EDA.ipynb for a more in-depth EDA exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f26e1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first 10 rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6456712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set passenger id as the index\n",
    "train_df.set_index('PassengerId',inplace=True)\n",
    "test_df.set_index('PassengerId',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33533008",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "The name and ticket columns have been dropped as they contain all unique values and appear unlikely to be useful to the model. The cabin column has a large number of nan values and so this has also been excluded.\n",
    "\n",
    "Made the same change on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name includes title which could be extracted using regex. This could also be an indicator (Mr, Mrs, Miss, Master, Dr., Rev.) \n",
    "# for example, indicates whether the passenger is married or not.  \n",
    "#train_df['Title'] = train_df['Name'].str.extract('([a-zA-Z]{2,}[\\.]{1})')\n",
    "#test_df['Title'] = test_df['Name'].str.extract('([a-zA-Z]{2,}[\\.]{1})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider whether to bin fields, like age for example, into categories. Age is unlikley to be useful as a continuous variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c65b97",
   "metadata": {},
   "source": [
    "**Clean dataset**\n",
    "\n",
    "There are 177 missing values in the Age column. Due to the small dataset size, we shouldn't remove all these rows. Instead use an imputer to replace missing values with a mean for the column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "102a53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop redundant columns in test and train sets\n",
    "cleaned_train_df = train_df.drop(columns=['Name','Ticket','Cabin']).copy()\n",
    "cleaned_test_df = test_df.drop(columns=['Name','Ticket','Cabin']).copy()\n",
    "\n",
    "# impute values where missing for Age and Embarked (train)\n",
    "imp_age = SimpleImputer(strategy='mean').fit(cleaned_train_df['Age'].values.reshape(-1,1))\n",
    "cleaned_train_df['Age'] = imp_age.transform(cleaned_train_df['Age'].values.reshape(-1,1))\n",
    "cleaned_test_df['Age'] = imp_age.transform(cleaned_test_df['Age'].values.reshape(-1,1))\n",
    "\n",
    "# impute values where missing for Age and Embarked (test)\n",
    "#imp_embarked = SimpleImputer(strategy='constant',fill_value='UNKNOWN').fit(cleaned_train_df['Embarked'].values.reshape(-1,1))\n",
    "#cleaned_train_df['Embarked'] = imp_embarked.transform(cleaned_train_df['Embarked'].values.reshape(-1,1))\n",
    "#cleaned_test_df['Embarked'] = imp_embarked.transform(cleaned_test_df['Embarked'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613d625",
   "metadata": {},
   "source": [
    "In the training set, there are two rows now with missing values in the Embarked column. These can be dropped as we will not lose too much data. Similarly, there is a missing value in the Fare column in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ed9193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where missing values are remaining\n",
    "cleaned_train_df.dropna(inplace=True)\n",
    "cleaned_test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88967838",
   "metadata": {},
   "source": [
    "**Transform Categoric data**\n",
    "\n",
    "Use One hot encoding to convert the categoric variables (sex, embarked, parch, Pclass, SibSp) so each category is a feature (1/0) \n",
    "\n",
    "This has to be done for both test and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8ddc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode categoric variables\n",
    "train_ohe_df = pd.get_dummies(cleaned_train_df,columns=['Sex','Embarked','Parch','Pclass','SibSp'])\n",
    "test_ohe_df = pd.get_dummies(cleaned_test_df,columns=['Sex','Embarked','Parch','Pclass','SibSp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb62581",
   "metadata": {},
   "source": [
    "**Transform Numeric data**\n",
    "\n",
    "Any fields containing continuous numeric data should be scaled or normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ea90660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and apply a scaler to the training set for the Age variable\n",
    "age_scaler = StandardScaler().fit(train_ohe_df['Age'].values.reshape(-1, 1))\n",
    "train_ohe_df['Age'] = age_scaler.transform(train_ohe_df['Age'].values.reshape(-1, 1))\n",
    "\n",
    "# fit and apply a scaler to the train set for the Fare variable\n",
    "fare_scaler = StandardScaler().fit(train_ohe_df['Fare'].values.reshape(-1, 1))\n",
    "train_ohe_df['Fare'] = age_scaler.transform(train_ohe_df['Fare'].values.reshape(-1, 1))\n",
    "\n",
    "# apply the scalers to the test set\n",
    "test_ohe_df['Age'] = age_scaler.transform(test_ohe_df['Age'].values.reshape(-1, 1))\n",
    "test_ohe_df['Fare'] = fare_scaler.transform(test_ohe_df['Fare'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7ed80",
   "metadata": {},
   "source": [
    "**Create feature and target datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00912369",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['Survived']\n",
    "feature_cols=[f for f in train_ohe_df.columns.to_list() if f not in target_col]\n",
    "\n",
    "X = train_ohe_df[feature_cols].copy()\n",
    "y = train_ohe_df[target_col].copy()\n",
    "\n",
    "X_inf = test_ohe_df[feature_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc274d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived\n",
       "PassengerId          \n",
       "1                   0\n",
       "2                   1\n",
       "3                   1\n",
       "4                   1\n",
       "5                   0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92682ffd",
   "metadata": {},
   "source": [
    "### Model training, validation and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0056ea4",
   "metadata": {},
   "source": [
    "Follow the training, validation and testing framework\n",
    " - Use 60% of the training data for model training\n",
    " - Use 20% for assessing model performance while fine-tuning parameters\n",
    " - Hold-out 20% for final evaluation\n",
    " \n",
    "Cross-validation should be applied to the 80% for training and fine-tuning using sklearn's GridSearchCV. This will also allow hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65d9a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into training and test sets for fitting and evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=324, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f271554",
   "metadata": {},
   "source": [
    "Train and validate each classifer on the training dataset using cross-validation. Then, score using the evaluation function. \n",
    "Accuracy is being used as the evaluation metric, with a dummy classifier to benchmark the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79d5a1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy training score: 0.6174402250351617\n",
      "Dummy test score: 0.6179775280898876\n"
     ]
    }
   ],
   "source": [
    "# define a dummy classifier to benchmark the evaluation\n",
    "dummy_model = DummyClassifier(strategy='most_frequent').fit(X_train,y_train)\n",
    "print(f'Dummy training score: {dummy_model.score(X_train,y_train)}')\n",
    "print(f'Dummy test score: {dummy_model.score(X_test,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb639be",
   "metadata": {},
   "source": [
    "Create a function for consistent training, validation and testing throughout experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccbe7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = '../models'\n",
    "def model_train_val(model, params, X_train, y_train, X_test, y_test, cv=4, scoring='accuracy'):\n",
    "    clf = GridSearchCV(model, param_grid=params, cv=cv, scoring=scoring, return_train_score=True)\n",
    "    clf.fit(X_train, y_train.values.ravel())\n",
    "    print(f'Best params: {clf.best_params_}')\n",
    "    print(f'Best CV score: {clf.best_score_}')\n",
    "    print(f\"Training set score: {clf.cv_results_['mean_train_score'][clf.best_index_]:2.3}\")\n",
    "    print(f'Test set score: {clf.best_estimator_.score(X_test,y_test):2.3}')\n",
    "    joblib.dump(clf.best_estimator_,os.path.join(model_repo, f'model_{str(clf.best_estimator_)}.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9c613",
   "metadata": {},
   "source": [
    "#### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "262de373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_neighbors': 7}\n",
      "Best CV score: 0.8115676379102392\n",
      "Training set score: 0.836\n",
      "Test set score: 0.792\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbors\n",
    "params = {'n_neighbors':range(2,11)}\n",
    "model_train_val(KNeighborsClassifier(), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330d75a",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73a4b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 10, 'penalty': 'l2'}\n",
      "Best CV score: 0.8059496603821494\n",
      "Training set score: 0.815\n",
      "Test set score: 0.803\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "params = {'penalty':['none','l1','l2'],'C':[0.01,0.1,1,10]}\n",
    "model_train_val(LogisticRegression(), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621b968",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76a4069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 1, 'kernel': 'linear'}\n",
      "Best CV score: 0.7862311940582746\n",
      "Training set score: 0.799\n",
      "Test set score: 0.798\n"
     ]
    }
   ],
   "source": [
    "# support vector machines\n",
    "params = {'kernel':('linear','rbf'),'C':(0.01,0.1,1)}\n",
    "model_train_val(SVC(), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db76d0",
   "metadata": {},
   "source": [
    "#### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "params = {'max_depth':range(1,11),'max_features':range(1,10)}\n",
    "model_train_val(DecisionTreeClassifier(random_state=0), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8f76f",
   "metadata": {},
   "source": [
    "#### Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce657028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forests\n",
    "params = {'n_estimators':range(1,101,10),'max_depth':range(1,11),}\n",
    "model_train_val(RandomForestClassifier(random_state=0), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d75e0",
   "metadata": {},
   "source": [
    "#### Gradient boosted trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d3182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient boosted trees\n",
    "params= {'n_estimators':range(1,101,10),'max_depth':range(1,11)}\n",
    "model_train_val(GradientBoostingClassifier(random_state=0), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using XGBoost\n",
    "params= {'n_estimators':range(1,101,10),'max_depth':range(1,11),'max_features':range(1,10)}\n",
    "#model_train_val(XGBClassifier(random_state=0,use_label_encoder=False), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa2476",
   "metadata": {},
   "source": [
    "### Model diagnostic\n",
    "\n",
    "With the exception of logistic regression, the above scores show that the model is overfitting to the training data in each case. Training set scores are higher than test set scores (high variance so the models are too complex).  \n",
    "\n",
    "Approaches we can take:\n",
    " - Use less features\n",
    " - Get more training examples (not possible here)\n",
    " - Increase regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78a412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-titanic",
   "language": "python",
   "name": "venv-titanic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
