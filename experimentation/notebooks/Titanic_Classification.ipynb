{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3f7dc46",
   "metadata": {},
   "source": [
    "# Titanic dataset: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1759aa3",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "This notebook sets out an approach for building a survival-classifier predictive model for the Titanic disaster. Various models will be trained and evaluated using data from Kaggle. The best-performer will then be deployed for use. \n",
    "\n",
    "Goal: create a machine learning model to generate predictions for whether an individual will survive the titanic disaster.\n",
    "\n",
    "\n",
    "\n",
    "**Feature descriptors:**\n",
    " - Pclass: ticket class\n",
    " - Name: full name of passenger\n",
    " - Sex: sex (m/f)\n",
    " - Age: age in years\n",
    " - SibSp: # of siblings/spouses aboard\n",
    " - Parch: # of parents / children aboard the Titanic\n",
    " - Ticket: ticket number\n",
    " - Fare: passenger fare\n",
    " - Cabin: cabin number\n",
    " - Embarked: port of embarkation\n",
    " \n",
    " \n",
    "Target: Survival - whether the individual survived (0 - No, 1 - Yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb76f05",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ffa2a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c010e",
   "metadata": {},
   "source": [
    "**Load datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bdbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../datasets/train.csv\")\n",
    "test_df = pd.read_csv(\"../datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19278e6a",
   "metadata": {},
   "source": [
    "**Explore training data**\n",
    "\n",
    "see Titanic_EDA.ipynb for a more in-depth EDA exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f26e1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first 10 rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6456712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set passenger id as the index\n",
    "train_df.set_index('PassengerId',inplace=True)\n",
    "test_df.set_index('PassengerId',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33533008",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "The name and ticket columns have been dropped as they contain all unique values and appear unlikely to be useful to the model. The cabin column has a large number of nan values and so this has also been excluded. Then, the remaining rows with nan values have been dropped.\n",
    "\n",
    "Made the same change on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b6d0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name includes title which could be extracted using regex. This could also be an indicator (Mr, Mrs, Miss, Master, Dr., Rev.) \n",
    "# for example, indicates whether the passenger is married or not.  \n",
    "#train_df['Title'] = train_df['Name'].str.extract('([a-zA-Z]{2,}[\\.]{1})')\n",
    "#test_df['Title'] = test_df['Name'].str.extract('([a-zA-Z]{2,}[\\.]{1})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b80b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe with cleaned dataset\n",
    "cleaned_train_df = train_df.drop(columns=['Name','Ticket','Cabin']).dropna(axis=0).copy()\n",
    "cleaned_test_df = test_df.drop(columns=['Name','Ticket','Cabin']).dropna(axis=0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eec6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is there another way to use the nan values other than dropping them?\n",
    "# cabin could be a good indicator as likely encoded the location on the ship of the passenger\n",
    "# Consider whether to bin fields, like age for example, into categories. Age is unlikley to be useful as a continuous variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88967838",
   "metadata": {},
   "source": [
    "**Categoric data**\n",
    "\n",
    "Use One hot encoding to convert the categoric variables (sex, embarked, parch, Pclass, SibSp) so each category is a feature (1/0) \n",
    "\n",
    "This has to be done for both test and training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ddc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode categoric variables\n",
    "train_ohe_df = pd.get_dummies(cleaned_train_df,columns=['Sex','Embarked','Parch','Pclass','SibSp'])\n",
    "test_ohe_df = pd.get_dummies(cleaned_test_df,columns=['Sex','Embarked','Parch','Pclass','SibSp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb62581",
   "metadata": {},
   "source": [
    "**Numeric data**\n",
    "\n",
    "Any fields containing continuous numeric data should be scaled or normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea90660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and apply a scaler to the training set for the Age variable\n",
    "age_scaler = StandardScaler().fit(train_ohe_df['Age'].values.reshape(-1, 1))\n",
    "train_ohe_df['Age'] = age_scaler.transform(train_ohe_df['Age'].values.reshape(-1, 1))\n",
    "\n",
    "# fit and apply a scaler to the test set for the Fare variable\n",
    "fare_scaler = StandardScaler().fit(train_ohe_df['Fare'].values.reshape(-1, 1))\n",
    "train_ohe_df['Fare'] = age_scaler.transform(train_ohe_df['Fare'].values.reshape(-1, 1))\n",
    "\n",
    "# apply the scaler to the test set\n",
    "test_ohe_df['Age'] = age_scaler.transform(test_ohe_df['Age'].values.reshape(-1, 1))\n",
    "test_ohe_df['Fare'] = fare_scaler.transform(test_ohe_df['Fare'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc7ed80",
   "metadata": {},
   "source": [
    "**Create feature and target datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00912369",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['Survived']\n",
    "feature_cols=[f for f in train_ohe_df.columns.to_list() if f not in target_col]\n",
    "\n",
    "X = train_ohe_df[feature_cols].copy()\n",
    "y = train_ohe_df[target_col].copy()\n",
    "\n",
    "X_inf = test_ohe_df[feature_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92682ffd",
   "metadata": {},
   "source": [
    "### Model training, validation and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0056ea4",
   "metadata": {},
   "source": [
    "Follow the training, validation and testing framework\n",
    " - use 60% of the training data for model training\n",
    " - use 20% for assessing model performance while fine-tuning parameters\n",
    " - hold-out 20% for final evaluation\n",
    " \n",
    "Cross-validation should be applied to the 80% for training and fine-tuning using sklearn's GridSearchCV. This will also allow hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d9a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into training and test sets for fitting and evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=324)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d6576",
   "metadata": {},
   "source": [
    "**Evaluation**: consider what metric should this model be optimised for. \n",
    "\n",
    "Create functions for consistent evaluation throughout model experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50c7bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an evaluation function for repetitive evaluation (accuracy only)\n",
    "def model_eval_acc(model, X_train, y_train, X_test, y_test):\n",
    "    print(f\"Training score: {model.score(X_train,y_train):2.3}\")\n",
    "    print(f\"Test score: {model.score(X_test,y_test):2.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9368468e",
   "metadata": {},
   "source": [
    "Accuracy is being used in this case, with a dummy classifier to benchmark the evaluation with a bad score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79d5a1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.592\n",
      "Test score: 0.608\n"
     ]
    }
   ],
   "source": [
    "# define a dummy classifier to benchmark the evaluation\n",
    "dummy_model = DummyClassifier(strategy='most_frequent').fit(X_train,y_train)\n",
    "model_eval_acc(dummy_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f271554",
   "metadata": {},
   "source": [
    "**Train ML model**: train and validate each classifer on the training dataset using cross-validation. Then, score using the evaluation function. \n",
    "\n",
    "Create a function for consistent training, validation and testing throughout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ccbe7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = '../models'\n",
    "def model_train_val(model, params, X_train, y_train, X_test, y_test, cv=4, scoring='accuracy'):\n",
    "    clf = GridSearchCV(model, param_grid=params, cv=cv, scoring=scoring, return_train_score=True)\n",
    "    clf.fit(X_train, y_train.values.ravel())\n",
    "    print(f'Best params: {clf.best_params_}')\n",
    "    print(f'Best CV score: {clf.best_score_}')\n",
    "    print(f\"Training set score: {clf.cv_results_['mean_train_score'][clf.best_index_]:2.3}\")\n",
    "    print(f'Test set score: {clf.best_estimator_.score(X_test,y_test):2.3}')\n",
    "    joblib.dump(clf.best_estimator_,os.path.join(model_repo, f'model_{str(clf.best_estimator_)}.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9c613",
   "metadata": {},
   "source": [
    "Firstly, try kNN, logistic regression and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "262de373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'n_neighbors': 7}\n",
      "Best CV score: 0.8172338225155126\n",
      "Training set score: 0.849\n",
      "Test set score: 0.811\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbors\n",
    "params = {'n_neighbors':range(2,11)}\n",
    "model_train_val(KNeighborsClassifier(), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "73a4b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 1, 'penalty': 'l2'}\n",
      "Best CV score: 0.7908007485472275\n",
      "Training set score: 0.816\n",
      "Test set score: 0.811\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "params = {'penalty':['none','l1','l2'],'C':[0.01,0.1,1,10]}\n",
    "model_train_val(LogisticRegression(), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "76a4069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 1, 'kernel': 'rbf'}\n",
      "Best CV score: 0.7767408647690338\n",
      "Training set score: 0.801\n",
      "Test set score: 0.804\n"
     ]
    }
   ],
   "source": [
    "# support vector machines\n",
    "params = {'kernel':('linear','rbf'),'C':(0.01,0.1,1)}\n",
    "model_train_val(SVC(), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db76d0",
   "metadata": {},
   "source": [
    "Try Decision Trees, Random forests and gradient-boosted trees (and XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "56b9ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 7, 'max_features': 8}\n",
      "Best CV score: 0.8101546340982961\n",
      "Training set score: 0.9\n",
      "Test set score: 0.72\n"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "params = {'max_depth':range(1,11),'max_features':range(1,10)}\n",
    "model_train_val(DecisionTreeClassifier(random_state=0), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ce657028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 10, 'n_estimators': 71}\n",
      "Best CV score: 0.8154363242391411\n",
      "Training set score: 0.959\n",
      "Test set score: 0.762\n"
     ]
    }
   ],
   "source": [
    "# random forests\n",
    "params = {'n_estimators':range(1,101,10),'max_depth':range(1,11),}\n",
    "model_train_val(RandomForestClassifier(random_state=0), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e5d3182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.8259504579927115\n",
      "Best params: {'max_depth': 3, 'n_estimators': 91}\n",
      "Training set score: 0.917\n",
      "Test set score: 0.79\n"
     ]
    }
   ],
   "source": [
    "# gradient boosted trees\n",
    "params= {'n_estimators':range(1,101,10),'max_depth':range(1,11)}\n",
    "model_train_val(GradientBoostingClassifier(random_state=0), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "020e002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using XGBoost\n",
    "params= {'n_estimators':range(1,101,10),'max_depth':range(1,11),'max_features':range(1,10)}\n",
    "#model_train_val(XGBClassifier(random_state=0,use_label_encoder=False), params, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc81a06",
   "metadata": {},
   "source": [
    "Try using deep learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef02db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10fa2476",
   "metadata": {},
   "source": [
    "### Model diagnostic\n",
    "\n",
    "With the exception of logistic regression, the above scores show that the model is overfitting to the training data in each case. Training set scores are higher than test set scores (high variance so the models are too complex).  \n",
    "\n",
    "Approaches we can take:\n",
    " - Use less features\n",
    " - Get more training examples (not possible here)\n",
    " - Increase regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78a412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-titanic",
   "language": "python",
   "name": "venv-titanic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
